# AI Evals and Experimentation Platform for Creative AI - Technical Overview

## 1. System Architecture

The platform is a full-stack web application designed to evaluate and compare the performance of different Large Language Models (LLMs) on creative editing tasks. It leverages a Python FastAPI backend for orchestration and AI integration, and a React frontend for the user interface, with Supabase serving as the persistent data store.

### Technology Stack

*   **Frontend**: React (Vite), TypeScript, Tailwind CSS.
*   **Backend**: Python, FastAPI, Uvicorn.
*   **Database**: Supabase (PostgreSQL).
*   **AI Providers**:
    *   **OpenAI**: GPT-4o (Reasoning/Editing), Whisper-1 (Transcription).
    *   **Anthropic**: Claude 3 Haiku (Editing).
*   **Infrastructure**: Local development environment (Windows), environment variable management via `.env`.

---

## 2. Core Workflows

### 2.1 Experiment Initialization (`POST /v1/experiment`)
1.  **Input Handling**: Accepts an audio file (`.mp3`, `.wav`) or raw text input.
2.  **Transcription**: If audio is provided, it is temporarily saved and processed by OpenAI's Whisper model (`transcribe_audio` in `services.py`) to generate a transcript.
3.  **Experiment Creation**: An entry is created in the `experiments` Supabase table with status `RUNNING`.
4.  **Background Processing**: The `OrchestrationEngine` runs the experiment flow asynchronously (`BackgroundTasks`).

### 2.2 Orchestration Flow (`src/orchestrator.py`)
The `OrchestrationEngine` manages the lifecycle of an experiment:
1.  **Model Execution**: Iterates through selected models (up to 3).
    *   **OpenAI**: Uses standard `openai` Python client.
    *   **Anthropic**: Uses specific `anthropic` client version (`0.25.7`).
        *   **Critical Implementation Detail**: The `api.py` module explicitly unsets `HTTP_PROXY`, `HTTPS_PROXY`, and `ALL_PROXY` environment variables at runtime to prevents `httpx` and Anthropic SDK conflicts.
    *   **Error Handling**: Individual model failures are caught, logged, and persisted as "FAILED" runs with 0 scores to prevent the entire experiment from crashing.
2.  **Evaluation**: Immediately after a model run, the output is evaluated against heuristic metrics (`edit_quality`, `structural_clarity`, `publish_ready`).
3.  **Comparison**: Once all runs complete, logic determines a "winning" model based on the highest aggregate score.
4.  **Completion**: Updates experiment status to `AWAITING_DECISION`.

### 2.3 Human Decision Loop
1.  **Review**: User compares model outputs, costs, and latencies in the UI.
2.  **Decision**: User selects an action: `SHIP`, `ITERATE`, or `ROLLBACK` and provides reasoning.
3.  **Submission (`POST /v1/decision`)**:
    *   Validation ensures the decision matches backend Enums (normalized to lowercase).
    *   Updates the `experiments` table with the decision and reason.
    *   Marks status as `COMPLETE`.

---

## 3. API Reference

### Experiment Management
*   `POST /v1/experiment`: Start a new experiment with file/text and model config.
*   `GET /v1/experiment/{experiment_id}`: Polls for status, partial results, and final recommendation.
*   `GET /v1/experiments`: Lists recent experiment history.
*   `GET /v1/experiments/{experiment_id}/details`: Fetches full details including run inputs, outputs, and specific metrics.

### Utilities
*   `POST /v1/transcribe`: Standalone transcription endpoint.
*   `POST /v1/decision`: Submits human judgment for an experiment.

---

## 4. Database Schema (Supabase)

### `experiments`
*   `experiment_id`: UUID (PK)
*   `media_id`: Identifier for the input media.
*   `status`: Enum (`running`, `failed`, `awaiting_decision`, `complete`)
*   `recommendation`: The model identifier of the automated winner.
*   `recommendation_reason`: Logic for why the model won.
*   `decision`: Human decision (`ship`, `iterate`, `rollback`).
*   `decision_reason`: Human reasoning text.

### `model_runs`
*   `run_id`: UUID (PK)
*   `experiment_id`: FK to `experiments`.
*   `model_name`: Identifier of the model (e.g., `gpt-4o`).
*   `raw_output`: The actual text generated by the model.
*   `latency_ms`: Execution time in milliseconds.
*   `cost_usd`: Estimated cost of the run.

### `eval_metrics`
*   `eval_id`: UUID (PK)
*   `run_id`: FK to `model_runs`.
*   `scores`: JSONB array containing metric objects:
    *   `metric_name`
    *   `score` (1-5)
    *   `reasoning`

---

## 5. Frontend Structure (`ui/`)

*   **`App.tsx`**: Main controller. Handles view routing (`new`, `list`, `detail`), state polling, and optimistic UI updates for decisions.
*   **`components/`**:
    *   `Sidebar.tsx`: Navigation.
    *   `UploadCard.tsx`: Drag-and-drop file upload.
    *   `ModelConfigCard.tsx`: Configuration for comparison slots (API Keys, Model Names).
    *   `ResultsCard.tsx`: Displays comparative table of metrics (Edit Effort, Quality, Cost, Latency).
    *   `DecisionCard.tsx`: Form for submitting human judgment or displaying past decisions.
    *   `ExperimentsList.tsx`: History table.
    *   `ExperimentDetailView.tsx`: Detailed drill-down view.

## 6. Known Implementation Details & Constraints
*   **Anthropic Versioning**: Pinned to `anthropic==0.25.7` and `httpx==0.27.2` to resolve proxy/client initialization issues.
*   **Heuristic Evaluation**: Currently uses a deterministic rule-based scorer (length, structure checks) in `AIProviderService.evaluate_output` rather than a secondary LLM for evaluation.
*   **Demo Mode Defaults**:
    *   Anthropic model hardcoded to `claude-3-haiku-20240307` for reliable free-tier access.
    *   OpenAI model routes to `gpt-4o`.
